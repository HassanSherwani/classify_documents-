{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.7 (tensorflow)",
      "language": "python",
      "name": "tensorflow"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "2.Advanced_Model.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "XwtZ0MegFbWg",
        "M8aNqsLFFbWt",
        "26F5gNOVFbXF"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0EB-00JFbTu",
        "colab_type": "text"
      },
      "source": [
        "# Advanced ML Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgD89GH8FbTv",
        "colab_type": "text"
      },
      "source": [
        "# 1)- Import key modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPdEp39wFbTw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# support both Python 2 and Python 3 with minimal overhead.\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "# I am an engineer. I care only about error not warning. So, let's be maverick and ignore warnings.\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRdpb4goFbT0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re    # for regular expressions \n",
        "import nltk  # for text manipulation \n",
        "import string \n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import string \n",
        "\n",
        "#For Visuals\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "from matplotlib import rcParams\n",
        "rcParams['figure.figsize'] = 11, 8\n",
        "%config InlineBackend.figure_format = 'svg'\n",
        "%matplotlib inline"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWdqIo5eFbT3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#models and evaluation\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from nltk.classify.scikitlearn import SklearnClassifier # notice its from ntlk not sklearn\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "# Evaluation packages\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcY1ltBZF3bY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "outputId": "7307a33c-44b3-4ac7-a112-629254c2db4c"
      },
      "source": [
        "!pip install version_information"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting version_information\n",
            "  Downloading https://files.pythonhosted.org/packages/ff/b0/6088e15b9ac43a08ccd300d68e0b900a20cf62077596c11ad11dd8cc9e4b/version_information-1.0.3.tar.gz\n",
            "Building wheels for collected packages: version-information\n",
            "  Building wheel for version-information (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for version-information: filename=version_information-1.0.3-cp36-none-any.whl size=3881 sha256=44d29efe59f9c47608b85a9ec3c7386a47cc545f025f1a97653e126a86cc0a20\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/4c/b3/1976ac11dbd802723b564de1acaa453a72c36c95827e576321\n",
            "Successfully built version-information\n",
            "Installing collected packages: version-information\n",
            "Successfully installed version-information-1.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRhRcERtFbT7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "b2aed913-1639-4e12-b215-841512a8750e"
      },
      "source": [
        "#pip install version_information\n",
        "%reload_ext version_information\n",
        "%version_information pandas,numpy, nltk, seaborn, matplotlib"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/latex": "\\begin{tabular}{|l|l|}\\hline\n{\\bf Software} & {\\bf Version} \\\\ \\hline\\hline\nPython & 3.6.9 64bit [GCC 8.4.0] \\\\ \\hline\nIPython & 5.5.0 \\\\ \\hline\nOS & Linux 4.19.112+ x86\\_64 with Ubuntu 18.04 bionic \\\\ \\hline\npandas & 1.0.5 \\\\ \\hline\nnumpy & 1.18.5 \\\\ \\hline\nnltk & 3.2.5 \\\\ \\hline\nseaborn & 0.10.1 \\\\ \\hline\nmatplotlib & 3.2.2 \\\\ \\hline\n\\hline \\multicolumn{2}{|l|}{Fri Sep 25 11:48:12 2020 UTC} \\\\ \\hline\n\\end{tabular}\n",
            "application/json": {
              "Software versions": [
                {
                  "version": "3.6.9 64bit [GCC 8.4.0]",
                  "module": "Python"
                },
                {
                  "version": "5.5.0",
                  "module": "IPython"
                },
                {
                  "version": "Linux 4.19.112+ x86_64 with Ubuntu 18.04 bionic",
                  "module": "OS"
                },
                {
                  "version": "1.0.5",
                  "module": "pandas"
                },
                {
                  "version": "1.18.5",
                  "module": "numpy"
                },
                {
                  "version": "3.2.5",
                  "module": "nltk"
                },
                {
                  "version": "0.10.1",
                  "module": "seaborn"
                },
                {
                  "version": "3.2.2",
                  "module": "matplotlib"
                }
              ]
            },
            "text/html": [
              "<table><tr><th>Software</th><th>Version</th></tr><tr><td>Python</td><td>3.6.9 64bit [GCC 8.4.0]</td></tr><tr><td>IPython</td><td>5.5.0</td></tr><tr><td>OS</td><td>Linux 4.19.112+ x86_64 with Ubuntu 18.04 bionic</td></tr><tr><td>pandas</td><td>1.0.5</td></tr><tr><td>numpy</td><td>1.18.5</td></tr><tr><td>nltk</td><td>3.2.5</td></tr><tr><td>seaborn</td><td>0.10.1</td></tr><tr><td>matplotlib</td><td>3.2.2</td></tr><tr><td colspan='2'>Fri Sep 25 11:48:12 2020 UTC</td></tr></table>"
            ],
            "text/plain": [
              "Software versions\n",
              "Python 3.6.9 64bit [GCC 8.4.0]\n",
              "IPython 5.5.0\n",
              "OS Linux 4.19.112+ x86_64 with Ubuntu 18.04 bionic\n",
              "pandas 1.0.5\n",
              "numpy 1.18.5\n",
              "nltk 3.2.5\n",
              "seaborn 0.10.1\n",
              "matplotlib 3.2.2\n",
              "Fri Sep 25 11:48:12 2020 UTC"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BbV39JbF5bl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ce09ebdb-2a3b-402c-97bd-f68f6b1d4aec"
      },
      "source": [
        "# testing GPU on colab\n",
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "''"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJdgcSFTFbT-",
        "colab_type": "text"
      },
      "source": [
        "# 2)- Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5gG3WTYFbUA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bb7c1566-a452-4018-de09-e6224e46e4f3"
      },
      "source": [
        "data=pd.read_csv('train_data_clean.csv')\n",
        "#data=data.rename(columns={'Unnamed: 0':'random_columns'}) # a trick to tackle random index values\n",
        "data.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bz9eBJgLFbUG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "85878f77-7f7f-4ab2-c316-92fee4d8dcd7"
      },
      "source": [
        "data.head(2)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>news</th>\n",
              "      <th>category</th>\n",
              "      <th>clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Top 5 Reasons Why 'Divergent' Star Kate Winsle...</td>\n",
              "      <td>e</td>\n",
              "      <td>top 5 reason diverg star kate winslet deserv s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Vessyl Bottle Tracks Your Drink And Its Health...</td>\n",
              "      <td>t</td>\n",
              "      <td>vessyl bottl track drink health benefitsgadget...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                news  ...                                              clean\n",
              "0  Top 5 Reasons Why 'Divergent' Star Kate Winsle...  ...  top 5 reason diverg star kate winslet deserv s...\n",
              "1  Vessyl Bottle Tracks Your Drink And Its Health...  ...  vessyl bottl track drink health benefitsgadget...\n",
              "\n",
              "[2 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xFzLpbSGJcL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "bbd49b6a-4bae-4b8e-df24-23c599476f93"
      },
      "source": [
        "data.isnull().sum()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "news        0\n",
              "category    0\n",
              "clean       0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vOYa_SMGwxG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#loading test feature and label data saved from previous notebooks\n",
        "feature_test=pd.read_csv('test_data.csv')\n",
        "label_test=pd.read_csv('test_label.csv')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EetJbAEyG0ve",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "eb0cea6a-3f95-462c-facf-15f5ecc547e4"
      },
      "source": [
        "print(feature_test.shape)\n",
        "print(label_test.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(84484, 2)\n",
            "(84484, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKt6gOTkFbUc",
        "colab_type": "text"
      },
      "source": [
        "# 3)- Vectorization\n",
        "\n",
        "- bag of words\n",
        "- tf-idf\n",
        "- doc2vec\n",
        "- word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShFfXjBmFbUl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "5ca4ae60-b057-4cb4-b56a-11b46b974171"
      },
      "source": [
        "features=data['clean']\n",
        "labels=data['category']\n",
        "print(features.shape)\n",
        "print(labels.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000,)\n",
            "(10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VQNqyhaFbUp",
        "colab_type": "text"
      },
      "source": [
        "### 3.1).Bag of Words\n",
        "\n",
        "Bag-of-Words is a method to represent text into numerical features.\n",
        "\n",
        "Let us understand this using a simple example. Suppose we have only 2 document\n",
        "\n",
        "- D1: He is a lazy boy. She is also lazy.\n",
        "\n",
        "- D2: Smith is a lazy person.\n",
        "\n",
        "The list created would consist of all the unique tokens in the corpus C.\n",
        "\n",
        "= [‘He’,’She’,’lazy’,’boy’,’Smith’,’person’]\n",
        "\n",
        "Here, D=2, N=6\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZzIES8cFbUq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f57fff56-4dad-4b25-cab9-c7b4f8562d6e"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "import gensim\n",
        "\n",
        "bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
        "# bag-of-words feature matrix\n",
        "bow = bow_vectorizer.fit_transform(features)\n",
        "bow.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 1000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3l-p5QNMe1K",
        "colab_type": "text"
      },
      "source": [
        "#### 3.1.a. Transform test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHBkiisSMdwH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "b3e03457-9ee0-45d8-e894-bae8358967b9"
      },
      "source": [
        "feature_test.head(2)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>153245</td>\n",
              "      <td>iPhone 6 Release Date Pushed Back Due to Issue...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>308611</td>\n",
              "      <td>Samsung Galaxy S4 vs Galaxy S3: Budget-Friendl...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0                                              title\n",
              "0      153245  iPhone 6 Release Date Pushed Back Due to Issue...\n",
              "1      308611  Samsung Galaxy S4 vs Galaxy S3: Budget-Friendl..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X15rD6fIMd7c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bow_test = bow_vectorizer.transform(feature_test[\"title\"])"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxMDMLx7MeHs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "276a787c-c4c4-44eb-b99f-2a41bcb18a89"
      },
      "source": [
        "print(bow.shape)\n",
        "print(bow_test.shape)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 1000)\n",
            "(84484, 1000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iG1YMKlTFbUt",
        "colab_type": "text"
      },
      "source": [
        "### 3.2)-TF-IDF\n",
        "\n",
        "This is another method which is based on the frequency method but it is different to the bag-of-words approach in the sense that it takes into account not just the occurrence of a word in a single document (or tweet) but in the entire corpus.\n",
        "\n",
        "TF-IDF works by penalising the common words by assigning them lower weights while giving importance to words which are rare in the entire corpus but appear in good numbers in few documents.\n",
        "\n",
        "Let’s have a look at the important terms related to TF-IDF:\n",
        "\n",
        "- TF = (Number of times term t appears in a document)/(Number of terms in the document)\n",
        "\n",
        "- IDF = log(N/n), where, N is the number of documents and n is the number of documents a term t has appeared in.\n",
        "\n",
        "- TF-IDF = TF*IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmBmDmJwFbUu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "258c7911-792a-4439-ae34-bd9f73c474a3"
      },
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
        "# TF-IDF feature matrix\n",
        "tfidf = tfidf_vectorizer.fit_transform(features)\n",
        "tfidf.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 1000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kv4t_vJdMzbz",
        "colab_type": "text"
      },
      "source": [
        "#### 3.2.a. Transform Test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtrSZpXkM3bv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf_test = tfidf_vectorizer.transform(feature_test[\"title\"])"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPomwNbtM3fp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "55e118ba-8a5f-49a3-cecc-ef4c560bf3f1"
      },
      "source": [
        "print(tfidf.shape)\n",
        "print(tfidf_test.shape)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 1000)\n",
            "(84484, 1000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uugYJWZrFbUy",
        "colab_type": "text"
      },
      "source": [
        "### 3.3)- Doc2Vec Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnItz5KrFbUy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm \n",
        "tqdm.pandas(desc=\"progress-bar\") \n",
        "from gensim.models.doc2vec import TaggedDocument"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IYkbNZTHgv8",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fK2gRZ9PFbU1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenized_text = data['clean'].apply(lambda x: x.split()) # tokenizing"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLbq7ImhFbU5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_label(twt):\n",
        "    output = []\n",
        "    for i, s in zip(twt.index, twt):\n",
        "        output.append(TaggedDocument(s, [\"clean_\" + str(i)]))\n",
        "    return output\n",
        "labeled_text = add_label(tokenized_text) # label all the news"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbQjox6DFbU8",
        "colab_type": "text"
      },
      "source": [
        "##### 3.3.a.Train doc2vec model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XQwLPzOFbU9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_d2v = gensim.models.Doc2Vec(dm=1,dm_mean=1,vector_size=200,window=5,negative=7,min_count=5,workers=3,alpha=0.1,seed=23)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJydoaBGFbU_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c1234397-5282-44e4-ecc0-19eddfe6308a"
      },
      "source": [
        "model_d2v.build_vocab([i for i in tqdm(labeled_text)])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [00:00<00:00, 1460920.93it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0yLmMlpFbVB",
        "colab_type": "text"
      },
      "source": [
        "##### 3.3.b.Preparing doc2vec Feature Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XF4puIMZFbVC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "00714aa2-b325-4d8b-8377-2f7fed3d3d1b"
      },
      "source": [
        "docvec_arrays = np.zeros((len(tokenized_text), 200))\n",
        "for i in range(len(data)):\n",
        "    docvec_arrays[i,:] = model_d2v.docvecs[i].reshape((1,200))\n",
        "\n",
        "    \n",
        "docvec_df = pd.DataFrame(docvec_arrays)\n",
        "docvec_df.shape"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 200)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQQ0LflTFbVE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "0588517d-cc25-4960-9a2b-6cfe9b495b12"
      },
      "source": [
        "docvec_df.head()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>160</th>\n",
              "      <th>161</th>\n",
              "      <th>162</th>\n",
              "      <th>163</th>\n",
              "      <th>164</th>\n",
              "      <th>165</th>\n",
              "      <th>166</th>\n",
              "      <th>167</th>\n",
              "      <th>168</th>\n",
              "      <th>169</th>\n",
              "      <th>170</th>\n",
              "      <th>171</th>\n",
              "      <th>172</th>\n",
              "      <th>173</th>\n",
              "      <th>174</th>\n",
              "      <th>175</th>\n",
              "      <th>176</th>\n",
              "      <th>177</th>\n",
              "      <th>178</th>\n",
              "      <th>179</th>\n",
              "      <th>180</th>\n",
              "      <th>181</th>\n",
              "      <th>182</th>\n",
              "      <th>183</th>\n",
              "      <th>184</th>\n",
              "      <th>185</th>\n",
              "      <th>186</th>\n",
              "      <th>187</th>\n",
              "      <th>188</th>\n",
              "      <th>189</th>\n",
              "      <th>190</th>\n",
              "      <th>191</th>\n",
              "      <th>192</th>\n",
              "      <th>193</th>\n",
              "      <th>194</th>\n",
              "      <th>195</th>\n",
              "      <th>196</th>\n",
              "      <th>197</th>\n",
              "      <th>198</th>\n",
              "      <th>199</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000660</td>\n",
              "      <td>-0.001548</td>\n",
              "      <td>-0.001803</td>\n",
              "      <td>0.001017</td>\n",
              "      <td>0.000443</td>\n",
              "      <td>0.002277</td>\n",
              "      <td>-0.001327</td>\n",
              "      <td>0.000648</td>\n",
              "      <td>-0.001661</td>\n",
              "      <td>-0.002412</td>\n",
              "      <td>-0.001085</td>\n",
              "      <td>-0.001390</td>\n",
              "      <td>0.001621</td>\n",
              "      <td>0.000651</td>\n",
              "      <td>-0.001689</td>\n",
              "      <td>0.000301</td>\n",
              "      <td>0.002134</td>\n",
              "      <td>-0.001802</td>\n",
              "      <td>-0.000452</td>\n",
              "      <td>-0.002162</td>\n",
              "      <td>0.002090</td>\n",
              "      <td>0.001161</td>\n",
              "      <td>-0.002306</td>\n",
              "      <td>0.002419</td>\n",
              "      <td>0.000466</td>\n",
              "      <td>-0.002420</td>\n",
              "      <td>0.001162</td>\n",
              "      <td>-0.000503</td>\n",
              "      <td>0.002183</td>\n",
              "      <td>-0.001155</td>\n",
              "      <td>-0.002186</td>\n",
              "      <td>-0.000369</td>\n",
              "      <td>-0.001165</td>\n",
              "      <td>-0.000774</td>\n",
              "      <td>0.000329</td>\n",
              "      <td>-0.000395</td>\n",
              "      <td>0.000179</td>\n",
              "      <td>0.000970</td>\n",
              "      <td>-0.000976</td>\n",
              "      <td>-0.001376</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000464</td>\n",
              "      <td>0.000956</td>\n",
              "      <td>-0.000348</td>\n",
              "      <td>0.000980</td>\n",
              "      <td>-0.001252</td>\n",
              "      <td>0.001497</td>\n",
              "      <td>0.000941</td>\n",
              "      <td>0.000353</td>\n",
              "      <td>-0.001991</td>\n",
              "      <td>0.002098</td>\n",
              "      <td>-0.001090</td>\n",
              "      <td>0.000263</td>\n",
              "      <td>0.001332</td>\n",
              "      <td>-0.000861</td>\n",
              "      <td>-0.001793</td>\n",
              "      <td>-0.001753</td>\n",
              "      <td>-0.000463</td>\n",
              "      <td>0.000376</td>\n",
              "      <td>-0.000585</td>\n",
              "      <td>-0.002241</td>\n",
              "      <td>-0.001413</td>\n",
              "      <td>-0.000529</td>\n",
              "      <td>0.002321</td>\n",
              "      <td>-0.002379</td>\n",
              "      <td>-0.002320</td>\n",
              "      <td>0.000869</td>\n",
              "      <td>-0.001165</td>\n",
              "      <td>-0.001717</td>\n",
              "      <td>0.001700</td>\n",
              "      <td>-0.000151</td>\n",
              "      <td>0.001759</td>\n",
              "      <td>0.001252</td>\n",
              "      <td>-0.000431</td>\n",
              "      <td>-0.002377</td>\n",
              "      <td>-0.000306</td>\n",
              "      <td>0.001925</td>\n",
              "      <td>0.001344</td>\n",
              "      <td>-0.001755</td>\n",
              "      <td>0.002436</td>\n",
              "      <td>0.000104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.000643</td>\n",
              "      <td>-0.001514</td>\n",
              "      <td>0.002060</td>\n",
              "      <td>0.001986</td>\n",
              "      <td>0.001788</td>\n",
              "      <td>-0.002209</td>\n",
              "      <td>0.002283</td>\n",
              "      <td>0.001975</td>\n",
              "      <td>0.002193</td>\n",
              "      <td>-0.001416</td>\n",
              "      <td>-0.002301</td>\n",
              "      <td>0.000521</td>\n",
              "      <td>-0.002120</td>\n",
              "      <td>0.000802</td>\n",
              "      <td>0.000530</td>\n",
              "      <td>-0.001273</td>\n",
              "      <td>0.000675</td>\n",
              "      <td>0.001101</td>\n",
              "      <td>0.002246</td>\n",
              "      <td>-0.001894</td>\n",
              "      <td>0.001171</td>\n",
              "      <td>-0.002045</td>\n",
              "      <td>0.002220</td>\n",
              "      <td>0.001133</td>\n",
              "      <td>-0.000770</td>\n",
              "      <td>-0.000855</td>\n",
              "      <td>0.000913</td>\n",
              "      <td>-0.000234</td>\n",
              "      <td>0.002405</td>\n",
              "      <td>-0.001193</td>\n",
              "      <td>-0.001934</td>\n",
              "      <td>0.001232</td>\n",
              "      <td>-0.001200</td>\n",
              "      <td>0.001148</td>\n",
              "      <td>-0.000680</td>\n",
              "      <td>-0.000302</td>\n",
              "      <td>0.000863</td>\n",
              "      <td>0.001703</td>\n",
              "      <td>-0.002086</td>\n",
              "      <td>-0.000216</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001424</td>\n",
              "      <td>-0.000645</td>\n",
              "      <td>0.002287</td>\n",
              "      <td>0.001414</td>\n",
              "      <td>0.001195</td>\n",
              "      <td>0.000979</td>\n",
              "      <td>-0.000099</td>\n",
              "      <td>-0.001730</td>\n",
              "      <td>-0.001844</td>\n",
              "      <td>-0.000229</td>\n",
              "      <td>0.002394</td>\n",
              "      <td>-0.001560</td>\n",
              "      <td>-0.002325</td>\n",
              "      <td>-0.000574</td>\n",
              "      <td>-0.001903</td>\n",
              "      <td>0.001161</td>\n",
              "      <td>-0.001391</td>\n",
              "      <td>0.000984</td>\n",
              "      <td>-0.000118</td>\n",
              "      <td>-0.001652</td>\n",
              "      <td>0.001745</td>\n",
              "      <td>0.001118</td>\n",
              "      <td>0.002353</td>\n",
              "      <td>-0.001233</td>\n",
              "      <td>0.001241</td>\n",
              "      <td>-0.002389</td>\n",
              "      <td>-0.001753</td>\n",
              "      <td>0.001194</td>\n",
              "      <td>-0.001765</td>\n",
              "      <td>-0.000146</td>\n",
              "      <td>0.001008</td>\n",
              "      <td>0.000385</td>\n",
              "      <td>-0.000687</td>\n",
              "      <td>0.001901</td>\n",
              "      <td>-0.000519</td>\n",
              "      <td>0.001515</td>\n",
              "      <td>-0.000411</td>\n",
              "      <td>0.001046</td>\n",
              "      <td>-0.001441</td>\n",
              "      <td>-0.001738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.001979</td>\n",
              "      <td>0.001849</td>\n",
              "      <td>-0.001995</td>\n",
              "      <td>-0.001026</td>\n",
              "      <td>-0.000852</td>\n",
              "      <td>-0.001799</td>\n",
              "      <td>0.000509</td>\n",
              "      <td>0.000860</td>\n",
              "      <td>-0.001535</td>\n",
              "      <td>0.001565</td>\n",
              "      <td>0.001100</td>\n",
              "      <td>0.001211</td>\n",
              "      <td>0.001209</td>\n",
              "      <td>-0.001482</td>\n",
              "      <td>0.001661</td>\n",
              "      <td>-0.001420</td>\n",
              "      <td>-0.001368</td>\n",
              "      <td>0.000574</td>\n",
              "      <td>-0.001618</td>\n",
              "      <td>-0.001350</td>\n",
              "      <td>0.002393</td>\n",
              "      <td>0.000925</td>\n",
              "      <td>-0.000395</td>\n",
              "      <td>0.000259</td>\n",
              "      <td>0.001515</td>\n",
              "      <td>0.001906</td>\n",
              "      <td>0.001472</td>\n",
              "      <td>0.000751</td>\n",
              "      <td>-0.001331</td>\n",
              "      <td>0.000112</td>\n",
              "      <td>0.000217</td>\n",
              "      <td>-0.000464</td>\n",
              "      <td>0.001808</td>\n",
              "      <td>0.001030</td>\n",
              "      <td>-0.001323</td>\n",
              "      <td>-0.002057</td>\n",
              "      <td>0.000434</td>\n",
              "      <td>-0.000212</td>\n",
              "      <td>-0.001170</td>\n",
              "      <td>0.002322</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.001776</td>\n",
              "      <td>-0.001900</td>\n",
              "      <td>0.001152</td>\n",
              "      <td>-0.002029</td>\n",
              "      <td>0.001571</td>\n",
              "      <td>-0.000752</td>\n",
              "      <td>-0.002377</td>\n",
              "      <td>-0.000784</td>\n",
              "      <td>0.002026</td>\n",
              "      <td>0.001351</td>\n",
              "      <td>0.000444</td>\n",
              "      <td>0.000966</td>\n",
              "      <td>0.000085</td>\n",
              "      <td>-0.000400</td>\n",
              "      <td>0.001452</td>\n",
              "      <td>-0.001162</td>\n",
              "      <td>0.000999</td>\n",
              "      <td>0.000947</td>\n",
              "      <td>0.001177</td>\n",
              "      <td>0.002333</td>\n",
              "      <td>0.001800</td>\n",
              "      <td>0.000455</td>\n",
              "      <td>0.001037</td>\n",
              "      <td>0.000276</td>\n",
              "      <td>-0.002273</td>\n",
              "      <td>0.002066</td>\n",
              "      <td>-0.001888</td>\n",
              "      <td>-0.001703</td>\n",
              "      <td>0.002245</td>\n",
              "      <td>-0.001162</td>\n",
              "      <td>-0.000733</td>\n",
              "      <td>-0.000553</td>\n",
              "      <td>-0.000359</td>\n",
              "      <td>0.000163</td>\n",
              "      <td>0.000528</td>\n",
              "      <td>-0.001528</td>\n",
              "      <td>-0.000377</td>\n",
              "      <td>0.001272</td>\n",
              "      <td>-0.001139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000833</td>\n",
              "      <td>0.001087</td>\n",
              "      <td>-0.000411</td>\n",
              "      <td>-0.000663</td>\n",
              "      <td>-0.001789</td>\n",
              "      <td>-0.001146</td>\n",
              "      <td>-0.001027</td>\n",
              "      <td>-0.002483</td>\n",
              "      <td>0.000070</td>\n",
              "      <td>0.001170</td>\n",
              "      <td>-0.001924</td>\n",
              "      <td>0.000602</td>\n",
              "      <td>-0.000384</td>\n",
              "      <td>0.001695</td>\n",
              "      <td>0.001532</td>\n",
              "      <td>-0.000288</td>\n",
              "      <td>-0.001623</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>-0.000955</td>\n",
              "      <td>0.001100</td>\n",
              "      <td>0.001058</td>\n",
              "      <td>-0.000009</td>\n",
              "      <td>0.001272</td>\n",
              "      <td>-0.001759</td>\n",
              "      <td>-0.001806</td>\n",
              "      <td>0.001676</td>\n",
              "      <td>0.000563</td>\n",
              "      <td>-0.002048</td>\n",
              "      <td>-0.001953</td>\n",
              "      <td>0.001511</td>\n",
              "      <td>0.000782</td>\n",
              "      <td>-0.000603</td>\n",
              "      <td>-0.000560</td>\n",
              "      <td>-0.002205</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.000474</td>\n",
              "      <td>0.002141</td>\n",
              "      <td>0.002306</td>\n",
              "      <td>-0.000497</td>\n",
              "      <td>0.002250</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000151</td>\n",
              "      <td>-0.000505</td>\n",
              "      <td>0.002163</td>\n",
              "      <td>0.000279</td>\n",
              "      <td>0.002288</td>\n",
              "      <td>0.001558</td>\n",
              "      <td>-0.001581</td>\n",
              "      <td>-0.002436</td>\n",
              "      <td>-0.000155</td>\n",
              "      <td>-0.001032</td>\n",
              "      <td>-0.002245</td>\n",
              "      <td>0.002293</td>\n",
              "      <td>0.000564</td>\n",
              "      <td>0.002472</td>\n",
              "      <td>0.000732</td>\n",
              "      <td>-0.002029</td>\n",
              "      <td>-0.000661</td>\n",
              "      <td>0.002025</td>\n",
              "      <td>0.000142</td>\n",
              "      <td>-0.001471</td>\n",
              "      <td>-0.000774</td>\n",
              "      <td>0.000375</td>\n",
              "      <td>0.000222</td>\n",
              "      <td>0.000909</td>\n",
              "      <td>0.000706</td>\n",
              "      <td>0.001184</td>\n",
              "      <td>-0.000113</td>\n",
              "      <td>0.000377</td>\n",
              "      <td>0.000201</td>\n",
              "      <td>-0.000262</td>\n",
              "      <td>-0.000498</td>\n",
              "      <td>-0.002466</td>\n",
              "      <td>0.000778</td>\n",
              "      <td>0.000853</td>\n",
              "      <td>-0.000561</td>\n",
              "      <td>0.002186</td>\n",
              "      <td>0.001363</td>\n",
              "      <td>0.002281</td>\n",
              "      <td>0.000959</td>\n",
              "      <td>0.001521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.000257</td>\n",
              "      <td>-0.001420</td>\n",
              "      <td>0.000425</td>\n",
              "      <td>0.001791</td>\n",
              "      <td>-0.001414</td>\n",
              "      <td>-0.000034</td>\n",
              "      <td>-0.001541</td>\n",
              "      <td>0.001569</td>\n",
              "      <td>-0.002313</td>\n",
              "      <td>0.002408</td>\n",
              "      <td>-0.000006</td>\n",
              "      <td>0.001897</td>\n",
              "      <td>0.002358</td>\n",
              "      <td>-0.000350</td>\n",
              "      <td>0.001947</td>\n",
              "      <td>-0.001627</td>\n",
              "      <td>-0.001613</td>\n",
              "      <td>-0.002186</td>\n",
              "      <td>-0.000157</td>\n",
              "      <td>0.000643</td>\n",
              "      <td>0.001653</td>\n",
              "      <td>-0.000728</td>\n",
              "      <td>0.002204</td>\n",
              "      <td>-0.001091</td>\n",
              "      <td>0.001301</td>\n",
              "      <td>-0.002140</td>\n",
              "      <td>0.000378</td>\n",
              "      <td>-0.000617</td>\n",
              "      <td>0.001685</td>\n",
              "      <td>0.000145</td>\n",
              "      <td>-0.002145</td>\n",
              "      <td>-0.001471</td>\n",
              "      <td>0.001601</td>\n",
              "      <td>0.000026</td>\n",
              "      <td>-0.001206</td>\n",
              "      <td>0.001618</td>\n",
              "      <td>0.001730</td>\n",
              "      <td>0.000245</td>\n",
              "      <td>-0.002497</td>\n",
              "      <td>0.001613</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000270</td>\n",
              "      <td>0.000291</td>\n",
              "      <td>-0.002008</td>\n",
              "      <td>-0.000504</td>\n",
              "      <td>0.002242</td>\n",
              "      <td>-0.002224</td>\n",
              "      <td>0.001040</td>\n",
              "      <td>-0.002050</td>\n",
              "      <td>0.000228</td>\n",
              "      <td>0.002462</td>\n",
              "      <td>0.002483</td>\n",
              "      <td>-0.000502</td>\n",
              "      <td>0.001079</td>\n",
              "      <td>-0.001243</td>\n",
              "      <td>-0.000257</td>\n",
              "      <td>-0.001698</td>\n",
              "      <td>-0.002161</td>\n",
              "      <td>0.002076</td>\n",
              "      <td>-0.000724</td>\n",
              "      <td>-0.000359</td>\n",
              "      <td>-0.000807</td>\n",
              "      <td>0.001816</td>\n",
              "      <td>-0.000223</td>\n",
              "      <td>0.000381</td>\n",
              "      <td>-0.002461</td>\n",
              "      <td>-0.002221</td>\n",
              "      <td>-0.000591</td>\n",
              "      <td>-0.002397</td>\n",
              "      <td>-0.001409</td>\n",
              "      <td>0.000707</td>\n",
              "      <td>0.001782</td>\n",
              "      <td>-0.002190</td>\n",
              "      <td>-0.002089</td>\n",
              "      <td>0.001718</td>\n",
              "      <td>-0.002062</td>\n",
              "      <td>-0.000048</td>\n",
              "      <td>0.000422</td>\n",
              "      <td>-0.001455</td>\n",
              "      <td>-0.000906</td>\n",
              "      <td>-0.001383</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 200 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0         1         2    ...       197       198       199\n",
              "0  0.000660 -0.001548 -0.001803  ... -0.001755  0.002436  0.000104\n",
              "1 -0.000643 -0.001514  0.002060  ...  0.001046 -0.001441 -0.001738\n",
              "2  0.001979  0.001849 -0.001995  ... -0.000377  0.001272 -0.001139\n",
              "3  0.000833  0.001087 -0.000411  ...  0.002281  0.000959  0.001521\n",
              "4 -0.000257 -0.001420  0.000425  ... -0.001455 -0.000906 -0.001383\n",
              "\n",
              "[5 rows x 200 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsU7YQSnKq1I",
        "colab_type": "text"
      },
      "source": [
        "#### 3.3.3.transform test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kja4-fxMLAKf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "81a327fc-9ee9-4b33-8c5b-1752dc3a1aa8"
      },
      "source": [
        "feature_test.head(2)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>153245</td>\n",
              "      <td>iPhone 6 Release Date Pushed Back Due to Issue...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>308611</td>\n",
              "      <td>Samsung Galaxy S4 vs Galaxy S3: Budget-Friendl...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0                                              title\n",
              "0      153245  iPhone 6 Release Date Pushed Back Due to Issue...\n",
              "1      308611  Samsung Galaxy S4 vs Galaxy S3: Budget-Friendl..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6b0uJzWKuEI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenized_text_test = feature_test['title'].apply(lambda x: x.split()) # tokenizing"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ac5GB3hdKuHX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labeled_text_test = add_label(tokenized_text_test) # label all the news"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NH6BlSjPKuM5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f0c42665-7b1f-4bdf-f96d-5f6391cd177d"
      },
      "source": [
        "docvec_arrays_test = np.zeros((len(tokenized_text_test), 200))\n",
        "for i in range(len(data)):\n",
        "    docvec_arrays_test[i,:] = model_d2v.docvecs[i].reshape((1,200))\n",
        "\n",
        "    \n",
        "docvec_df_test = pd.DataFrame(docvec_arrays_test)\n"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(84484, 200)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vvmUKHdKuVH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "40c963e0-4c53-4bec-8d93-9f5b1760cef7"
      },
      "source": [
        "print(docvec_df.shape) # training\n",
        "print(docvec_df_test.shape) # for testing"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 200)\n",
            "(84484, 200)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMSFv9OGME8x",
        "colab_type": "text"
      },
      "source": [
        "Notice I have only transformed test set and didn't train. So only my train set learns about vocab of corpus. My test model has only been transformed and learnt nothing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_tp2_ErFbVG",
        "colab_type": "text"
      },
      "source": [
        "### 3.4.Word2Vec Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqpPfRHnFbVG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5d07d38d-39e1-48e3-decf-961ce27f78fd"
      },
      "source": [
        "tokenized_text = data['clean'].apply(lambda x: x.split()) # tokenizing\n",
        "\n",
        "model_w2v = gensim.models.Word2Vec(\n",
        "            tokenized_text,\n",
        "            size=200, # desired no. of features/independent variables\n",
        "            window=5, # context window size\n",
        "            min_count=2,\n",
        "            sg = 1, # 1 for skip-gram model\n",
        "            hs = 0,\n",
        "            negative = 10, # for negative sampling i.e class with other types\n",
        "            workers= 2, # no.of cores\n",
        "            seed = 34) \n",
        "\n",
        "model_w2v.train(tokenized_text, total_examples= len(data['clean']), epochs=20)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1329685, 1633380)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtntKOSTFbVJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 874
        },
        "outputId": "89be100b-1750-461e-e904-70e8d5ca146d"
      },
      "source": [
        "model_w2v.wv['nasdaq']"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-4.19158429e-01, -2.29767099e-01, -2.00829958e-03,  4.46986109e-01,\n",
              "        2.00698435e-01,  9.71860439e-02,  4.15311158e-02,  2.58581072e-01,\n",
              "        3.71628642e-01,  3.26173812e-01, -8.91381223e-03,  1.84910446e-01,\n",
              "        2.54351079e-01,  1.06504513e-02, -6.44920617e-02, -3.30742985e-01,\n",
              "       -6.48779571e-02,  1.23355500e-01,  4.43420932e-02,  1.63406491e-01,\n",
              "        3.49484593e-01, -1.41000241e-01, -4.32326406e-01, -1.62403971e-01,\n",
              "        9.17873859e-01,  1.92707494e-01, -2.93042153e-01,  1.03319466e-01,\n",
              "        2.02471361e-01,  9.48425606e-02,  4.04774070e-01, -4.50274954e-03,\n",
              "        4.01588649e-01, -3.88885550e-02, -7.54421204e-02, -1.67875737e-01,\n",
              "       -4.73084062e-01, -9.19232517e-03,  2.94244200e-01,  4.66713347e-02,\n",
              "        3.12463224e-01, -7.01050907e-02,  2.46901587e-02, -2.88366258e-01,\n",
              "       -2.06862837e-01, -2.26560801e-01,  5.84619462e-01,  4.43947613e-01,\n",
              "        6.38083816e-02, -4.46176529e-01,  8.23984519e-02,  8.29339400e-03,\n",
              "       -1.95765402e-02,  9.34525877e-02,  2.35182017e-01, -2.73244798e-01,\n",
              "       -9.49604288e-02,  4.30143833e-01, -1.58479109e-01, -1.40787840e-01,\n",
              "        2.29080945e-01,  2.27063045e-01, -2.75559843e-01,  1.14682734e-01,\n",
              "       -1.64593026e-01, -3.88324738e-01,  3.31730247e-01,  7.48581812e-02,\n",
              "       -1.44549203e-03, -2.10635900e-01,  4.88520302e-02, -4.78407413e-01,\n",
              "       -1.48123786e-01, -1.18431285e-01,  2.16697425e-01,  2.89326727e-01,\n",
              "       -5.57520762e-02,  3.64891350e-01,  2.84293532e-01,  3.19498152e-01,\n",
              "        1.19282134e-01, -2.24456131e-01,  3.26798886e-01, -3.36398751e-01,\n",
              "        1.27547041e-01, -1.57555938e-01, -3.77424955e-01, -1.37266695e-01,\n",
              "       -3.20929773e-02,  1.94648325e-01, -7.71379694e-02, -2.28119895e-01,\n",
              "        2.08263636e-01, -1.67806447e-01, -5.54655530e-02, -2.02972203e-01,\n",
              "       -4.59750928e-02,  2.86565751e-01, -1.36244386e-01,  3.93301100e-01,\n",
              "        3.27096991e-02, -2.88328826e-01,  6.79509761e-03, -2.55989283e-02,\n",
              "        1.17368095e-01, -3.06252956e-01, -2.24277526e-01, -3.73410344e-01,\n",
              "       -3.04296345e-01,  5.31411827e-01,  3.93256953e-04,  1.89866632e-01,\n",
              "        5.18990681e-02, -1.04983762e-01, -8.52509961e-02,  5.37458360e-02,\n",
              "        6.52343258e-02, -5.01984060e-01,  2.84391940e-01,  3.25907916e-01,\n",
              "       -3.12603295e-01,  6.72653094e-02,  4.45450813e-01, -3.33598286e-01,\n",
              "        1.95689425e-01,  3.36430855e-02,  1.39955729e-01, -2.46399075e-01,\n",
              "        3.83267850e-01, -2.98600405e-01,  2.11845428e-01, -1.91837922e-01,\n",
              "        2.75129322e-02,  6.40273035e-01, -4.27325070e-01, -9.03578773e-02,\n",
              "        2.56215334e-01, -3.25356394e-01, -6.26152754e-02, -3.84183884e-01,\n",
              "       -8.91467705e-02, -4.61878598e-01,  1.40003711e-02, -4.56070304e-01,\n",
              "        2.88649052e-01, -1.21566065e-01,  3.47418904e-01, -3.77205849e-01,\n",
              "        4.11433905e-01,  9.10189450e-02, -1.23368897e-01, -1.53145105e-01,\n",
              "       -1.81864679e-01,  2.47498835e-03, -8.92451480e-02, -3.42581630e-01,\n",
              "       -2.28434339e-01, -2.42486537e-01, -5.21401539e-02, -6.67775795e-02,\n",
              "       -7.18839690e-02,  1.99810416e-01,  1.32759422e-01, -3.97054404e-01,\n",
              "       -1.28703281e-01,  4.40160751e-01, -6.09909773e-01,  2.16922939e-01,\n",
              "       -1.43908843e-01,  7.36605078e-02,  6.81080639e-01,  4.84740347e-01,\n",
              "       -8.14029723e-02, -5.39084226e-02,  2.55604148e-01,  1.57055438e-01,\n",
              "       -2.15756267e-01,  4.40120474e-02, -1.52721494e-01, -3.80831212e-01,\n",
              "        6.92126989e-01,  1.31560758e-01,  7.03274459e-02, -9.16279759e-03,\n",
              "        4.91853096e-02, -3.45681727e-01, -2.41077304e-01,  4.88921613e-01,\n",
              "       -3.56298715e-01,  5.44307008e-02, -3.02984983e-01,  4.56050694e-01,\n",
              "       -8.96729007e-02, -3.14326882e-02, -4.24049683e-02, -4.95611429e-02,\n",
              "       -7.28050768e-02, -1.77019984e-01, -1.60154458e-02,  6.93728030e-02],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fv9LHwFnFbVL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ea0aa8eb-9402-4051-e866-d4b462388642"
      },
      "source": [
        "len(model_w2v.wv['nasdaq'])"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtGMZsEtFbVO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5f598960-ce49-4af1-b4aa-a015feab87e7"
      },
      "source": [
        "type(model_w2v)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "gensim.models.word2vec.Word2Vec"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDptzTF5FbVR",
        "colab_type": "text"
      },
      "source": [
        "##### 3.4.1.Preparing Vectors for text data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6gYlrrnFbVR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word_vector(tokens, size):\n",
        "    vec = np.zeros(size).reshape((1, size))\n",
        "    count = 0.\n",
        "    for word in tokens:\n",
        "        try:\n",
        "            vec += model_w2v[word].reshape((1, size))\n",
        "            count += 1.\n",
        "        except KeyError: # handling the case where the token is not in vocabulary           \n",
        "            continue\n",
        "    if count != 0:\n",
        "        vec /= count\n",
        "    return vec"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsQ8RMMTFbVT",
        "colab_type": "text"
      },
      "source": [
        "##### 3.4.2.Preparing word2vec feature set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8PBWT-BFbVU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "8f2cf437-4f9f-4dc1-82e0-425804d150ab"
      },
      "source": [
        "wordvec_arrays = np.zeros((len(tokenized_text), 200)) \n",
        "for i in range(len(tokenized_text)):\n",
        "    wordvec_arrays[i,:] = word_vector(tokenized_text[i], 200)\n",
        "    wordvec_df = pd.DataFrame(wordvec_arrays)\n",
        "wordvec_df.head()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>160</th>\n",
              "      <th>161</th>\n",
              "      <th>162</th>\n",
              "      <th>163</th>\n",
              "      <th>164</th>\n",
              "      <th>165</th>\n",
              "      <th>166</th>\n",
              "      <th>167</th>\n",
              "      <th>168</th>\n",
              "      <th>169</th>\n",
              "      <th>170</th>\n",
              "      <th>171</th>\n",
              "      <th>172</th>\n",
              "      <th>173</th>\n",
              "      <th>174</th>\n",
              "      <th>175</th>\n",
              "      <th>176</th>\n",
              "      <th>177</th>\n",
              "      <th>178</th>\n",
              "      <th>179</th>\n",
              "      <th>180</th>\n",
              "      <th>181</th>\n",
              "      <th>182</th>\n",
              "      <th>183</th>\n",
              "      <th>184</th>\n",
              "      <th>185</th>\n",
              "      <th>186</th>\n",
              "      <th>187</th>\n",
              "      <th>188</th>\n",
              "      <th>189</th>\n",
              "      <th>190</th>\n",
              "      <th>191</th>\n",
              "      <th>192</th>\n",
              "      <th>193</th>\n",
              "      <th>194</th>\n",
              "      <th>195</th>\n",
              "      <th>196</th>\n",
              "      <th>197</th>\n",
              "      <th>198</th>\n",
              "      <th>199</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.112036</td>\n",
              "      <td>0.117529</td>\n",
              "      <td>0.214865</td>\n",
              "      <td>-0.020733</td>\n",
              "      <td>0.261518</td>\n",
              "      <td>0.107048</td>\n",
              "      <td>0.116260</td>\n",
              "      <td>-0.103589</td>\n",
              "      <td>0.398610</td>\n",
              "      <td>0.037055</td>\n",
              "      <td>0.262092</td>\n",
              "      <td>0.081014</td>\n",
              "      <td>0.009041</td>\n",
              "      <td>0.068648</td>\n",
              "      <td>-0.076937</td>\n",
              "      <td>-0.189877</td>\n",
              "      <td>0.096075</td>\n",
              "      <td>0.175835</td>\n",
              "      <td>0.056690</td>\n",
              "      <td>0.218068</td>\n",
              "      <td>0.127369</td>\n",
              "      <td>-0.100208</td>\n",
              "      <td>-0.288216</td>\n",
              "      <td>-0.004495</td>\n",
              "      <td>0.055956</td>\n",
              "      <td>-0.296565</td>\n",
              "      <td>-0.013570</td>\n",
              "      <td>-0.068814</td>\n",
              "      <td>0.006230</td>\n",
              "      <td>0.172260</td>\n",
              "      <td>0.215402</td>\n",
              "      <td>0.008420</td>\n",
              "      <td>0.393125</td>\n",
              "      <td>-0.000484</td>\n",
              "      <td>-0.026228</td>\n",
              "      <td>0.203838</td>\n",
              "      <td>0.034258</td>\n",
              "      <td>-0.043373</td>\n",
              "      <td>-0.089856</td>\n",
              "      <td>-0.004687</td>\n",
              "      <td>...</td>\n",
              "      <td>0.075618</td>\n",
              "      <td>0.123927</td>\n",
              "      <td>-0.085901</td>\n",
              "      <td>-0.130596</td>\n",
              "      <td>-0.066989</td>\n",
              "      <td>0.216751</td>\n",
              "      <td>-0.012993</td>\n",
              "      <td>-0.120950</td>\n",
              "      <td>-0.020062</td>\n",
              "      <td>-0.012617</td>\n",
              "      <td>0.139313</td>\n",
              "      <td>0.034025</td>\n",
              "      <td>-0.010441</td>\n",
              "      <td>-0.164775</td>\n",
              "      <td>0.214455</td>\n",
              "      <td>0.229792</td>\n",
              "      <td>-0.141525</td>\n",
              "      <td>-0.074406</td>\n",
              "      <td>0.065473</td>\n",
              "      <td>-0.097259</td>\n",
              "      <td>0.088345</td>\n",
              "      <td>-0.070987</td>\n",
              "      <td>0.107051</td>\n",
              "      <td>-0.219546</td>\n",
              "      <td>0.131598</td>\n",
              "      <td>-0.040460</td>\n",
              "      <td>-0.189088</td>\n",
              "      <td>-0.070738</td>\n",
              "      <td>-0.327408</td>\n",
              "      <td>0.138552</td>\n",
              "      <td>-0.161466</td>\n",
              "      <td>0.357012</td>\n",
              "      <td>-0.042573</td>\n",
              "      <td>-0.184069</td>\n",
              "      <td>0.227112</td>\n",
              "      <td>-0.004313</td>\n",
              "      <td>-0.108996</td>\n",
              "      <td>-0.163471</td>\n",
              "      <td>-0.103295</td>\n",
              "      <td>0.179760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.289337</td>\n",
              "      <td>-0.201637</td>\n",
              "      <td>-0.040488</td>\n",
              "      <td>0.012670</td>\n",
              "      <td>0.131444</td>\n",
              "      <td>0.172174</td>\n",
              "      <td>-0.124476</td>\n",
              "      <td>0.232219</td>\n",
              "      <td>0.263581</td>\n",
              "      <td>-0.041276</td>\n",
              "      <td>0.126665</td>\n",
              "      <td>0.166228</td>\n",
              "      <td>0.139223</td>\n",
              "      <td>-0.113769</td>\n",
              "      <td>0.155543</td>\n",
              "      <td>-0.156425</td>\n",
              "      <td>0.042678</td>\n",
              "      <td>0.173135</td>\n",
              "      <td>0.178740</td>\n",
              "      <td>0.095331</td>\n",
              "      <td>0.141841</td>\n",
              "      <td>0.007733</td>\n",
              "      <td>-0.170668</td>\n",
              "      <td>-0.169260</td>\n",
              "      <td>0.118065</td>\n",
              "      <td>0.129832</td>\n",
              "      <td>-0.003639</td>\n",
              "      <td>-0.086812</td>\n",
              "      <td>0.069391</td>\n",
              "      <td>0.176566</td>\n",
              "      <td>0.207020</td>\n",
              "      <td>0.069319</td>\n",
              "      <td>0.293605</td>\n",
              "      <td>0.016283</td>\n",
              "      <td>0.049575</td>\n",
              "      <td>0.106987</td>\n",
              "      <td>-0.018300</td>\n",
              "      <td>-0.189745</td>\n",
              "      <td>-0.030911</td>\n",
              "      <td>0.001348</td>\n",
              "      <td>...</td>\n",
              "      <td>0.112680</td>\n",
              "      <td>0.008730</td>\n",
              "      <td>0.079051</td>\n",
              "      <td>0.071825</td>\n",
              "      <td>-0.126326</td>\n",
              "      <td>0.294807</td>\n",
              "      <td>0.033628</td>\n",
              "      <td>-0.181058</td>\n",
              "      <td>-0.104755</td>\n",
              "      <td>-0.021393</td>\n",
              "      <td>-0.018796</td>\n",
              "      <td>-0.045801</td>\n",
              "      <td>0.034394</td>\n",
              "      <td>-0.044872</td>\n",
              "      <td>0.115539</td>\n",
              "      <td>0.126140</td>\n",
              "      <td>-0.090290</td>\n",
              "      <td>-0.262139</td>\n",
              "      <td>-0.014969</td>\n",
              "      <td>-0.100925</td>\n",
              "      <td>0.407028</td>\n",
              "      <td>-0.041355</td>\n",
              "      <td>-0.290562</td>\n",
              "      <td>0.073295</td>\n",
              "      <td>0.363646</td>\n",
              "      <td>0.107311</td>\n",
              "      <td>-0.276164</td>\n",
              "      <td>0.222163</td>\n",
              "      <td>-0.071557</td>\n",
              "      <td>0.131636</td>\n",
              "      <td>-0.058818</td>\n",
              "      <td>0.144118</td>\n",
              "      <td>0.134564</td>\n",
              "      <td>-0.228925</td>\n",
              "      <td>0.245931</td>\n",
              "      <td>-0.189699</td>\n",
              "      <td>-0.115568</td>\n",
              "      <td>-0.015011</td>\n",
              "      <td>0.015346</td>\n",
              "      <td>0.047497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.274414</td>\n",
              "      <td>0.008228</td>\n",
              "      <td>0.077097</td>\n",
              "      <td>0.177192</td>\n",
              "      <td>0.153487</td>\n",
              "      <td>0.133675</td>\n",
              "      <td>-0.036129</td>\n",
              "      <td>0.174917</td>\n",
              "      <td>0.179359</td>\n",
              "      <td>0.315468</td>\n",
              "      <td>0.087836</td>\n",
              "      <td>0.281910</td>\n",
              "      <td>0.078622</td>\n",
              "      <td>-0.041186</td>\n",
              "      <td>-0.280356</td>\n",
              "      <td>-0.019932</td>\n",
              "      <td>-0.172325</td>\n",
              "      <td>0.054286</td>\n",
              "      <td>-0.145681</td>\n",
              "      <td>-0.012326</td>\n",
              "      <td>0.064129</td>\n",
              "      <td>0.025044</td>\n",
              "      <td>-0.234353</td>\n",
              "      <td>-0.144449</td>\n",
              "      <td>0.155626</td>\n",
              "      <td>-0.202906</td>\n",
              "      <td>-0.010311</td>\n",
              "      <td>-0.140571</td>\n",
              "      <td>0.042945</td>\n",
              "      <td>0.284544</td>\n",
              "      <td>0.084359</td>\n",
              "      <td>0.191488</td>\n",
              "      <td>0.290815</td>\n",
              "      <td>-0.053890</td>\n",
              "      <td>0.170024</td>\n",
              "      <td>-0.017354</td>\n",
              "      <td>0.089014</td>\n",
              "      <td>-0.298681</td>\n",
              "      <td>0.193136</td>\n",
              "      <td>-0.103332</td>\n",
              "      <td>...</td>\n",
              "      <td>0.164183</td>\n",
              "      <td>0.128467</td>\n",
              "      <td>0.039750</td>\n",
              "      <td>-0.156851</td>\n",
              "      <td>-0.034757</td>\n",
              "      <td>0.059165</td>\n",
              "      <td>-0.207961</td>\n",
              "      <td>-0.266814</td>\n",
              "      <td>0.116116</td>\n",
              "      <td>0.065962</td>\n",
              "      <td>0.099356</td>\n",
              "      <td>-0.157603</td>\n",
              "      <td>-0.116380</td>\n",
              "      <td>-0.264511</td>\n",
              "      <td>0.176312</td>\n",
              "      <td>0.269113</td>\n",
              "      <td>0.071685</td>\n",
              "      <td>-0.171419</td>\n",
              "      <td>0.201537</td>\n",
              "      <td>0.053390</td>\n",
              "      <td>0.432342</td>\n",
              "      <td>0.049735</td>\n",
              "      <td>0.138381</td>\n",
              "      <td>-0.114735</td>\n",
              "      <td>0.022935</td>\n",
              "      <td>-0.030383</td>\n",
              "      <td>-0.222716</td>\n",
              "      <td>0.209642</td>\n",
              "      <td>-0.013833</td>\n",
              "      <td>-0.013324</td>\n",
              "      <td>0.047018</td>\n",
              "      <td>0.168587</td>\n",
              "      <td>0.147671</td>\n",
              "      <td>-0.200164</td>\n",
              "      <td>0.035668</td>\n",
              "      <td>0.072092</td>\n",
              "      <td>-0.089347</td>\n",
              "      <td>0.113562</td>\n",
              "      <td>0.012131</td>\n",
              "      <td>0.077801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.204454</td>\n",
              "      <td>0.129999</td>\n",
              "      <td>0.002591</td>\n",
              "      <td>0.184851</td>\n",
              "      <td>0.236623</td>\n",
              "      <td>0.098258</td>\n",
              "      <td>-0.072270</td>\n",
              "      <td>0.144204</td>\n",
              "      <td>0.332627</td>\n",
              "      <td>0.060820</td>\n",
              "      <td>0.353714</td>\n",
              "      <td>0.066546</td>\n",
              "      <td>0.333712</td>\n",
              "      <td>-0.096286</td>\n",
              "      <td>-0.243998</td>\n",
              "      <td>-0.075111</td>\n",
              "      <td>-0.012867</td>\n",
              "      <td>0.028415</td>\n",
              "      <td>-0.201648</td>\n",
              "      <td>0.018990</td>\n",
              "      <td>0.110832</td>\n",
              "      <td>-0.103295</td>\n",
              "      <td>-0.172666</td>\n",
              "      <td>-0.243262</td>\n",
              "      <td>-0.095308</td>\n",
              "      <td>-0.138136</td>\n",
              "      <td>0.084609</td>\n",
              "      <td>-0.037330</td>\n",
              "      <td>0.115271</td>\n",
              "      <td>0.224204</td>\n",
              "      <td>0.212095</td>\n",
              "      <td>0.118677</td>\n",
              "      <td>0.095768</td>\n",
              "      <td>-0.167019</td>\n",
              "      <td>0.237548</td>\n",
              "      <td>0.250463</td>\n",
              "      <td>-0.065403</td>\n",
              "      <td>-0.105358</td>\n",
              "      <td>0.002770</td>\n",
              "      <td>0.038685</td>\n",
              "      <td>...</td>\n",
              "      <td>0.274231</td>\n",
              "      <td>0.022653</td>\n",
              "      <td>-0.123800</td>\n",
              "      <td>-0.016609</td>\n",
              "      <td>0.076860</td>\n",
              "      <td>-0.146305</td>\n",
              "      <td>0.166027</td>\n",
              "      <td>-0.067783</td>\n",
              "      <td>-0.004471</td>\n",
              "      <td>0.029937</td>\n",
              "      <td>0.023908</td>\n",
              "      <td>-0.079308</td>\n",
              "      <td>0.028808</td>\n",
              "      <td>-0.196193</td>\n",
              "      <td>0.175206</td>\n",
              "      <td>0.216767</td>\n",
              "      <td>-0.231853</td>\n",
              "      <td>0.124330</td>\n",
              "      <td>0.077149</td>\n",
              "      <td>-0.076163</td>\n",
              "      <td>0.254819</td>\n",
              "      <td>0.201637</td>\n",
              "      <td>0.255593</td>\n",
              "      <td>-0.027024</td>\n",
              "      <td>0.210464</td>\n",
              "      <td>-0.293771</td>\n",
              "      <td>-0.232411</td>\n",
              "      <td>-0.047866</td>\n",
              "      <td>-0.052124</td>\n",
              "      <td>-0.062217</td>\n",
              "      <td>-0.167599</td>\n",
              "      <td>0.196851</td>\n",
              "      <td>0.022663</td>\n",
              "      <td>-0.309182</td>\n",
              "      <td>0.072871</td>\n",
              "      <td>-0.074297</td>\n",
              "      <td>-0.053123</td>\n",
              "      <td>-0.041199</td>\n",
              "      <td>0.149606</td>\n",
              "      <td>0.121862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.040696</td>\n",
              "      <td>-0.140997</td>\n",
              "      <td>0.048742</td>\n",
              "      <td>0.166069</td>\n",
              "      <td>0.012763</td>\n",
              "      <td>0.383296</td>\n",
              "      <td>-0.145714</td>\n",
              "      <td>0.233924</td>\n",
              "      <td>0.275225</td>\n",
              "      <td>0.106040</td>\n",
              "      <td>0.157928</td>\n",
              "      <td>-0.048313</td>\n",
              "      <td>0.547881</td>\n",
              "      <td>-0.317503</td>\n",
              "      <td>0.174900</td>\n",
              "      <td>-0.249315</td>\n",
              "      <td>0.020912</td>\n",
              "      <td>0.133484</td>\n",
              "      <td>-0.143742</td>\n",
              "      <td>0.225078</td>\n",
              "      <td>-0.103739</td>\n",
              "      <td>-0.202568</td>\n",
              "      <td>-0.218794</td>\n",
              "      <td>-0.214367</td>\n",
              "      <td>0.042929</td>\n",
              "      <td>-0.056102</td>\n",
              "      <td>-0.039667</td>\n",
              "      <td>-0.092803</td>\n",
              "      <td>-0.081320</td>\n",
              "      <td>0.141056</td>\n",
              "      <td>0.140662</td>\n",
              "      <td>0.103239</td>\n",
              "      <td>0.335180</td>\n",
              "      <td>-0.223149</td>\n",
              "      <td>0.064554</td>\n",
              "      <td>0.000190</td>\n",
              "      <td>-0.048593</td>\n",
              "      <td>-0.160125</td>\n",
              "      <td>0.044306</td>\n",
              "      <td>0.062820</td>\n",
              "      <td>...</td>\n",
              "      <td>0.141752</td>\n",
              "      <td>0.032901</td>\n",
              "      <td>0.292937</td>\n",
              "      <td>-0.123679</td>\n",
              "      <td>-0.079778</td>\n",
              "      <td>0.150132</td>\n",
              "      <td>-0.216592</td>\n",
              "      <td>-0.132141</td>\n",
              "      <td>-0.035940</td>\n",
              "      <td>0.136982</td>\n",
              "      <td>0.328401</td>\n",
              "      <td>0.000825</td>\n",
              "      <td>-0.218181</td>\n",
              "      <td>-0.134768</td>\n",
              "      <td>0.268109</td>\n",
              "      <td>0.197843</td>\n",
              "      <td>-0.102123</td>\n",
              "      <td>-0.187827</td>\n",
              "      <td>0.143860</td>\n",
              "      <td>-0.143092</td>\n",
              "      <td>0.382291</td>\n",
              "      <td>0.040996</td>\n",
              "      <td>-0.085234</td>\n",
              "      <td>0.042325</td>\n",
              "      <td>0.223079</td>\n",
              "      <td>-0.062700</td>\n",
              "      <td>-0.371686</td>\n",
              "      <td>0.094494</td>\n",
              "      <td>-0.011818</td>\n",
              "      <td>-0.064072</td>\n",
              "      <td>-0.232719</td>\n",
              "      <td>0.374637</td>\n",
              "      <td>-0.139446</td>\n",
              "      <td>-0.209834</td>\n",
              "      <td>0.046198</td>\n",
              "      <td>-0.163259</td>\n",
              "      <td>-0.051753</td>\n",
              "      <td>-0.061007</td>\n",
              "      <td>-0.185041</td>\n",
              "      <td>0.006535</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 200 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0         1         2    ...       197       198       199\n",
              "0  0.112036  0.117529  0.214865  ... -0.163471 -0.103295  0.179760\n",
              "1  0.289337 -0.201637 -0.040488  ... -0.015011  0.015346  0.047497\n",
              "2  0.274414  0.008228  0.077097  ...  0.113562  0.012131  0.077801\n",
              "3  0.204454  0.129999  0.002591  ... -0.041199  0.149606  0.121862\n",
              "4  0.040696 -0.140997  0.048742  ... -0.061007 -0.185041  0.006535\n",
              "\n",
              "[5 rows x 200 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IiyQB40FbVW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ddae3f6d-e84e-4c46-f4e4-6e81b5740578"
      },
      "source": [
        "wordvec_df.shape"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 200)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHAutyVQJIKJ",
        "colab_type": "text"
      },
      "source": [
        "#### 3.4.3.transform test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1AMurZuI-Mp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "ef2f2113-f940-465e-aa6b-a809c44ca2a3"
      },
      "source": [
        "feature_test.head(2)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>153245</td>\n",
              "      <td>iPhone 6 Release Date Pushed Back Due to Issue...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>308611</td>\n",
              "      <td>Samsung Galaxy S4 vs Galaxy S3: Budget-Friendl...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0                                              title\n",
              "0      153245  iPhone 6 Release Date Pushed Back Due to Issue...\n",
              "1      308611  Samsung Galaxy S4 vs Galaxy S3: Budget-Friendl..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd72wMEjI-T_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenized_text_test = feature_test['title'].apply(lambda x: x.split()) # tokenizing"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fduT1yupJz9_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "76586c92-c98b-4bcc-b38c-8f09104b09a3"
      },
      "source": [
        "wordvec_arrays = np.zeros((len(tokenized_text_test), 200)) \n",
        "for i in range(len(tokenized_text_test)):\n",
        "    wordvec_arrays[i,:] = word_vector(tokenized_text_test[i], 200)\n",
        "    wordvec_df_test = pd.DataFrame(wordvec_arrays)\n",
        "wordvec_df_test.head()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>160</th>\n",
              "      <th>161</th>\n",
              "      <th>162</th>\n",
              "      <th>163</th>\n",
              "      <th>164</th>\n",
              "      <th>165</th>\n",
              "      <th>166</th>\n",
              "      <th>167</th>\n",
              "      <th>168</th>\n",
              "      <th>169</th>\n",
              "      <th>170</th>\n",
              "      <th>171</th>\n",
              "      <th>172</th>\n",
              "      <th>173</th>\n",
              "      <th>174</th>\n",
              "      <th>175</th>\n",
              "      <th>176</th>\n",
              "      <th>177</th>\n",
              "      <th>178</th>\n",
              "      <th>179</th>\n",
              "      <th>180</th>\n",
              "      <th>181</th>\n",
              "      <th>182</th>\n",
              "      <th>183</th>\n",
              "      <th>184</th>\n",
              "      <th>185</th>\n",
              "      <th>186</th>\n",
              "      <th>187</th>\n",
              "      <th>188</th>\n",
              "      <th>189</th>\n",
              "      <th>190</th>\n",
              "      <th>191</th>\n",
              "      <th>192</th>\n",
              "      <th>193</th>\n",
              "      <th>194</th>\n",
              "      <th>195</th>\n",
              "      <th>196</th>\n",
              "      <th>197</th>\n",
              "      <th>198</th>\n",
              "      <th>199</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.058814</td>\n",
              "      <td>0.142585</td>\n",
              "      <td>0.162008</td>\n",
              "      <td>-0.131452</td>\n",
              "      <td>-0.038308</td>\n",
              "      <td>-0.093332</td>\n",
              "      <td>-0.295874</td>\n",
              "      <td>0.167939</td>\n",
              "      <td>0.437889</td>\n",
              "      <td>0.115090</td>\n",
              "      <td>0.536151</td>\n",
              "      <td>-0.226375</td>\n",
              "      <td>0.260178</td>\n",
              "      <td>-0.662223</td>\n",
              "      <td>0.517701</td>\n",
              "      <td>-0.154320</td>\n",
              "      <td>0.458935</td>\n",
              "      <td>0.054735</td>\n",
              "      <td>0.195445</td>\n",
              "      <td>0.134831</td>\n",
              "      <td>-0.054053</td>\n",
              "      <td>0.289312</td>\n",
              "      <td>-0.200386</td>\n",
              "      <td>0.586942</td>\n",
              "      <td>-0.012638</td>\n",
              "      <td>-0.562773</td>\n",
              "      <td>0.079707</td>\n",
              "      <td>-0.084097</td>\n",
              "      <td>0.307388</td>\n",
              "      <td>0.477308</td>\n",
              "      <td>0.110977</td>\n",
              "      <td>0.456154</td>\n",
              "      <td>0.755784</td>\n",
              "      <td>-0.264319</td>\n",
              "      <td>0.678109</td>\n",
              "      <td>0.115447</td>\n",
              "      <td>-0.200939</td>\n",
              "      <td>-1.109471</td>\n",
              "      <td>0.225328</td>\n",
              "      <td>-0.234797</td>\n",
              "      <td>...</td>\n",
              "      <td>0.249021</td>\n",
              "      <td>0.311836</td>\n",
              "      <td>0.292920</td>\n",
              "      <td>-0.136811</td>\n",
              "      <td>0.250913</td>\n",
              "      <td>-0.215157</td>\n",
              "      <td>-0.238375</td>\n",
              "      <td>-0.111332</td>\n",
              "      <td>-0.066764</td>\n",
              "      <td>0.463247</td>\n",
              "      <td>-0.335175</td>\n",
              "      <td>0.477282</td>\n",
              "      <td>0.250130</td>\n",
              "      <td>-0.698363</td>\n",
              "      <td>0.900268</td>\n",
              "      <td>0.402433</td>\n",
              "      <td>-0.209631</td>\n",
              "      <td>0.033896</td>\n",
              "      <td>-0.095080</td>\n",
              "      <td>-0.042442</td>\n",
              "      <td>0.332289</td>\n",
              "      <td>0.284670</td>\n",
              "      <td>-0.816700</td>\n",
              "      <td>0.140234</td>\n",
              "      <td>0.058792</td>\n",
              "      <td>0.313469</td>\n",
              "      <td>0.344483</td>\n",
              "      <td>-0.530620</td>\n",
              "      <td>-0.636006</td>\n",
              "      <td>-0.136761</td>\n",
              "      <td>-0.928272</td>\n",
              "      <td>0.757920</td>\n",
              "      <td>0.142069</td>\n",
              "      <td>0.069877</td>\n",
              "      <td>0.267336</td>\n",
              "      <td>0.061147</td>\n",
              "      <td>-0.060970</td>\n",
              "      <td>-0.430171</td>\n",
              "      <td>0.295875</td>\n",
              "      <td>0.204263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.043115</td>\n",
              "      <td>-0.974044</td>\n",
              "      <td>-0.206173</td>\n",
              "      <td>0.020041</td>\n",
              "      <td>0.107864</td>\n",
              "      <td>0.707798</td>\n",
              "      <td>0.023119</td>\n",
              "      <td>-0.241604</td>\n",
              "      <td>1.079093</td>\n",
              "      <td>-0.468236</td>\n",
              "      <td>0.138944</td>\n",
              "      <td>0.008469</td>\n",
              "      <td>-0.334753</td>\n",
              "      <td>-0.061533</td>\n",
              "      <td>0.272005</td>\n",
              "      <td>-0.344717</td>\n",
              "      <td>0.118476</td>\n",
              "      <td>0.517618</td>\n",
              "      <td>-0.425236</td>\n",
              "      <td>0.244206</td>\n",
              "      <td>0.491671</td>\n",
              "      <td>0.659521</td>\n",
              "      <td>-0.299715</td>\n",
              "      <td>0.288081</td>\n",
              "      <td>0.198810</td>\n",
              "      <td>-0.910492</td>\n",
              "      <td>-0.063048</td>\n",
              "      <td>-0.279178</td>\n",
              "      <td>-0.348123</td>\n",
              "      <td>0.156782</td>\n",
              "      <td>0.851360</td>\n",
              "      <td>-0.178772</td>\n",
              "      <td>0.745497</td>\n",
              "      <td>-0.229396</td>\n",
              "      <td>0.103883</td>\n",
              "      <td>0.048155</td>\n",
              "      <td>0.025691</td>\n",
              "      <td>0.109420</td>\n",
              "      <td>-0.368924</td>\n",
              "      <td>-0.198040</td>\n",
              "      <td>...</td>\n",
              "      <td>0.351654</td>\n",
              "      <td>-0.243385</td>\n",
              "      <td>0.447427</td>\n",
              "      <td>-0.110816</td>\n",
              "      <td>0.137341</td>\n",
              "      <td>0.149611</td>\n",
              "      <td>-0.041090</td>\n",
              "      <td>-0.342503</td>\n",
              "      <td>0.033549</td>\n",
              "      <td>-0.384458</td>\n",
              "      <td>0.425065</td>\n",
              "      <td>0.242884</td>\n",
              "      <td>-0.264038</td>\n",
              "      <td>-0.588206</td>\n",
              "      <td>1.102590</td>\n",
              "      <td>0.715110</td>\n",
              "      <td>0.036484</td>\n",
              "      <td>-0.450524</td>\n",
              "      <td>-0.442744</td>\n",
              "      <td>-0.242947</td>\n",
              "      <td>0.060046</td>\n",
              "      <td>0.141410</td>\n",
              "      <td>0.628185</td>\n",
              "      <td>-0.264588</td>\n",
              "      <td>0.198128</td>\n",
              "      <td>-0.347974</td>\n",
              "      <td>0.325655</td>\n",
              "      <td>0.507832</td>\n",
              "      <td>-0.161697</td>\n",
              "      <td>0.046850</td>\n",
              "      <td>-0.756813</td>\n",
              "      <td>0.462182</td>\n",
              "      <td>-0.082657</td>\n",
              "      <td>-0.189085</td>\n",
              "      <td>0.400665</td>\n",
              "      <td>0.242772</td>\n",
              "      <td>0.349228</td>\n",
              "      <td>-0.438436</td>\n",
              "      <td>0.276026</td>\n",
              "      <td>-0.106309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.172886</td>\n",
              "      <td>0.193834</td>\n",
              "      <td>-0.374816</td>\n",
              "      <td>-0.039309</td>\n",
              "      <td>-0.043196</td>\n",
              "      <td>-0.047114</td>\n",
              "      <td>-0.291135</td>\n",
              "      <td>0.436828</td>\n",
              "      <td>0.234712</td>\n",
              "      <td>-0.302049</td>\n",
              "      <td>0.188180</td>\n",
              "      <td>0.735383</td>\n",
              "      <td>-0.226910</td>\n",
              "      <td>0.278506</td>\n",
              "      <td>0.016267</td>\n",
              "      <td>-0.153429</td>\n",
              "      <td>-0.303333</td>\n",
              "      <td>-0.314978</td>\n",
              "      <td>-0.145325</td>\n",
              "      <td>0.002189</td>\n",
              "      <td>0.426099</td>\n",
              "      <td>-0.145265</td>\n",
              "      <td>-0.191935</td>\n",
              "      <td>-0.266227</td>\n",
              "      <td>0.458960</td>\n",
              "      <td>-0.091921</td>\n",
              "      <td>-0.118590</td>\n",
              "      <td>-0.475493</td>\n",
              "      <td>-0.014878</td>\n",
              "      <td>0.408133</td>\n",
              "      <td>-0.077138</td>\n",
              "      <td>0.267566</td>\n",
              "      <td>0.121506</td>\n",
              "      <td>0.199233</td>\n",
              "      <td>0.431362</td>\n",
              "      <td>-0.457671</td>\n",
              "      <td>-0.188031</td>\n",
              "      <td>-0.110527</td>\n",
              "      <td>0.312198</td>\n",
              "      <td>-0.060574</td>\n",
              "      <td>...</td>\n",
              "      <td>0.635516</td>\n",
              "      <td>0.193468</td>\n",
              "      <td>0.258531</td>\n",
              "      <td>0.035396</td>\n",
              "      <td>-0.047896</td>\n",
              "      <td>-0.085285</td>\n",
              "      <td>-0.652828</td>\n",
              "      <td>-0.477882</td>\n",
              "      <td>0.115442</td>\n",
              "      <td>-0.070614</td>\n",
              "      <td>0.167695</td>\n",
              "      <td>0.014712</td>\n",
              "      <td>-0.070170</td>\n",
              "      <td>-0.715051</td>\n",
              "      <td>0.170109</td>\n",
              "      <td>0.449853</td>\n",
              "      <td>-0.030339</td>\n",
              "      <td>-0.415414</td>\n",
              "      <td>0.391800</td>\n",
              "      <td>-0.171774</td>\n",
              "      <td>0.502936</td>\n",
              "      <td>0.046668</td>\n",
              "      <td>-0.027920</td>\n",
              "      <td>-0.059437</td>\n",
              "      <td>0.058433</td>\n",
              "      <td>-0.220778</td>\n",
              "      <td>-0.244991</td>\n",
              "      <td>0.402897</td>\n",
              "      <td>-0.147593</td>\n",
              "      <td>0.069189</td>\n",
              "      <td>-0.559469</td>\n",
              "      <td>0.830084</td>\n",
              "      <td>-0.003190</td>\n",
              "      <td>0.129910</td>\n",
              "      <td>0.352590</td>\n",
              "      <td>-0.120139</td>\n",
              "      <td>-0.240680</td>\n",
              "      <td>-0.458383</td>\n",
              "      <td>-0.236813</td>\n",
              "      <td>-0.098240</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 200 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0         1         2    ...       197       198       199\n",
              "0  0.058814  0.142585  0.162008  ... -0.430171  0.295875  0.204263\n",
              "1 -0.043115 -0.974044 -0.206173  ... -0.438436  0.276026 -0.106309\n",
              "2  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n",
              "3  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n",
              "4 -0.172886  0.193834 -0.374816  ... -0.458383 -0.236813 -0.098240\n",
              "\n",
              "[5 rows x 200 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_HxOoDzKLha",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "62321fd5-2640-4282-e976-75da8f5ce8ca"
      },
      "source": [
        "print(wordvec_df.shape)\n",
        "print(wordvec_df_test.shape)\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 200)\n",
            "(84484, 200)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsNzXAmuKR_o",
        "colab_type": "text"
      },
      "source": [
        "It may look weird as we have less data for train and more to test. But, training is computing intense. So, this will help us. Plus eventually I will train my best performing model to whole data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdnJk36HFbVY",
        "colab_type": "text"
      },
      "source": [
        "# 4)-Model Building\n",
        "\n",
        "- Logistic Regression \n",
        "- Support Vector\n",
        "- Random Forest\n",
        "- XGBoost\n",
        "- MLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZyB6Ih2FbVZ",
        "colab_type": "text"
      },
      "source": [
        "### 4.1.Logistic Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXwWhdpAFbVZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression \n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn.metrics import f1_score, accuracy_score, roc_curve,roc_auc_score,confusion_matrix, classification_report"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yb7nG3LxFbVc",
        "colab_type": "text"
      },
      "source": [
        "##### 4.1.a. Logistic Regression using Bag-of-Words Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iEJ9RfsFbVc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "5f9cf0a9-f692-47ce-e142-489727aa2941"
      },
      "source": [
        "X=bow\n",
        "y=data['category']\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "print(bow_test.shape)\n",
        "print(label_test.shape)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 1000)\n",
            "(10000,)\n",
            "(84484, 1000)\n",
            "(84484, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQPg-YXfFbVf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# splitting data into training and validation set\n",
        "#xtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(bow, y,random_state=42,test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jweLRi-aFbVj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "1e852b3b-bd8f-4d71-f141-d9e06185749e"
      },
      "source": [
        "lreg_bow = LogisticRegression(solver='liblinear')\n",
        "\n",
        "# training the model\n",
        "lreg_bow.fit(X, y)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImmHJhilFbVl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6aef7926-4628-498d-c69e-34ef9500af38"
      },
      "source": [
        "# predicting on the validation set\n",
        "prediction_bow = lreg_bow.predict_proba(bow_test)\n",
        "prediction_bow[0]"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.16087434, 0.6072539 , 0.04823555, 0.1836362 ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beAraqzwFbVn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c4c69802-7b42-4b3c-d884-e6a1b4492f03"
      },
      "source": [
        "# prediction over classes\n",
        "\n",
        "prediction_bow_class=lreg_bow.predict(bow_test)\n",
        "prediction_bow_class[0]"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'e'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGz9B4YfFbVp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "37c7cedb-87a1-49bd-a205-17bfdd359180"
      },
      "source": [
        "accuracy_score(label_test, prediction_bow_class)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7044765872827992"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4Dldo8nPaJR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "2154ef04-3fa1-4de7-fe8c-54b03b7df347"
      },
      "source": [
        "from sklearn import metrics\n",
        "print(metrics.classification_report(label_test, prediction_bow_class))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           b       0.73      0.65      0.69     23367\n",
            "           e       0.65      0.92      0.76     30300\n",
            "           m       0.75      0.47      0.58      9207\n",
            "           t       0.80      0.56      0.66     21610\n",
            "\n",
            "    accuracy                           0.70     84484\n",
            "   macro avg       0.73      0.65      0.67     84484\n",
            "weighted avg       0.72      0.70      0.70     84484\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBGrdDU_FbVr",
        "colab_type": "text"
      },
      "source": [
        "##### 4.1.b.Logistic Regression using TF-IDF Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfgu8I2iFbVs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "8908cebe-ed68-405b-9b71-bbc639955f72"
      },
      "source": [
        "X=tfidf\n",
        "y=data['category']\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "print(tfidf_test.shape)\n",
        "print(label_test.shape)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 1000)\n",
            "(10000,)\n",
            "(84484, 1000)\n",
            "(84484, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqNxBx1xFbVz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "cc2ef5c3-a7e0-4e16-d241-c29d48f1d601"
      },
      "source": [
        "lreg_tfidf = LogisticRegression(solver='liblinear')\n",
        "\n",
        "# training the model\n",
        "lreg_tfidf.fit(X, y)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-VNi8euFbV2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# predicting on the validation set\n",
        "prediction_tfidf = lreg_tfidf.predict_proba(tfidf_test)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQJRDvn2FbV5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction_tfidf_class=lreg_tfidf.predict(tfidf_test)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okZ87DBgFbV7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8896272a-b725-4d17-9066-5b4f77c2527d"
      },
      "source": [
        "accuracy_score(label_test, prediction_tfidf_class)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.707542256521945"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9KoqxK_Pp4d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "eecac841-4a8e-4ce4-bc13-c157185d7c01"
      },
      "source": [
        "print(metrics.classification_report(label_test, prediction_tfidf_class))"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           b       0.73      0.66      0.69     23367\n",
            "           e       0.66      0.92      0.77     30300\n",
            "           m       0.78      0.46      0.58      9207\n",
            "           t       0.79      0.57      0.66     21610\n",
            "\n",
            "    accuracy                           0.71     84484\n",
            "   macro avg       0.74      0.65      0.67     84484\n",
            "weighted avg       0.72      0.71      0.70     84484\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wbk9ltufFbV9",
        "colab_type": "text"
      },
      "source": [
        "##### 4.1.c. Logistic Regression using Word2Vec Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmI6mOKBQ2dC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "2d61b426-ec07-4d9e-8b76-efb7d67320f4"
      },
      "source": [
        "X=wordvec_df\n",
        "y=data['category']\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "print(wordvec_df_test.shape)\n",
        "print(label_test.shape)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 200)\n",
            "(10000,)\n",
            "(84484, 200)\n",
            "(84484, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9C1WP4QXFbWF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "f42ef802-6ce7-47d4-b727-d4a4f37d2ca2"
      },
      "source": [
        "lreg_word2vec = LogisticRegression(solver='liblinear')\n",
        "# training the model\n",
        "lreg_word2vec.fit(X, y)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LyeoWGCFbWH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# predicting on the validation set\n",
        "prediction_word2vec = lreg_word2vec.predict_proba(wordvec_df_test)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-BmMTfGFbWK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction_word2vec_class=lreg_word2vec.predict(wordvec_df_test)"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBF4vlsoFbWM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3b62b7b3-0855-44b7-9fce-d9db2b9102e7"
      },
      "source": [
        "accuracy_score(label_test, prediction_word2vec_class)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3385374745513943"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjtoZOewRy49",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "9fbc4aa7-8103-4552-e1ae-98e5a3a6cb21"
      },
      "source": [
        "print(metrics.classification_report(label_test, prediction_word2vec_class))"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           b       0.63      0.33      0.43     23367\n",
            "           e       0.58      0.30      0.40     30300\n",
            "           m       0.14      0.69      0.24      9207\n",
            "           t       0.45      0.25      0.32     21610\n",
            "\n",
            "    accuracy                           0.34     84484\n",
            "   macro avg       0.45      0.39      0.35     84484\n",
            "weighted avg       0.51      0.34      0.37     84484\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sg0Ym9JbFbWS",
        "colab_type": "text"
      },
      "source": [
        "##### 4.1.d. Logistic Regression using Doc2Vec Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZV37T5DCFbWT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "3eb1bf5b-7519-4638-e7a9-88fc8f59689d"
      },
      "source": [
        "X=docvec_df\n",
        "y=data['category']\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "print(docvec_df_test.shape)\n",
        "print(label_test.shape)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 200)\n",
            "(10000,)\n",
            "(84484, 200)\n",
            "(84484, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HA8Qd5_LFbWX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "faf6b85d-1fd0-4419-a103-7cd5bba2285a"
      },
      "source": [
        "lreg_doc2vec = LogisticRegression(solver='liblinear')\n",
        "# training the model\n",
        "lreg_doc2vec.fit(docvec_df, y)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qhmb_OTDFbWZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# predicting on the validation set\n",
        "prediction_doc2vec = lreg_doc2vec.predict_proba(docvec_df_test)"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGKC4_iIFbWb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction_doc2vec_class=lreg_doc2vec.predict(docvec_df_test)"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-iMtZoaFbWd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "10e5b7ce-f08c-4c2b-8ab3-d565097815c3"
      },
      "source": [
        "accuracy_score(label_test, prediction_doc2vec_class)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.35864779129776053"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwYQYPmSUiFl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "32c776b4-0b2f-445b-8540-53d45af155ce"
      },
      "source": [
        "print(metrics.classification_report(label_test, prediction_doc2vec_class))"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           b       0.00      0.00      0.00     23367\n",
            "           e       0.36      1.00      0.53     30300\n",
            "           m       0.00      0.00      0.00      9207\n",
            "           t       0.00      0.00      0.00     21610\n",
            "\n",
            "    accuracy                           0.36     84484\n",
            "   macro avg       0.09      0.25      0.13     84484\n",
            "weighted avg       0.13      0.36      0.19     84484\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxYIn4YHFbWf",
        "colab_type": "text"
      },
      "source": [
        "**Summary:**\n",
        "\n",
        "- bow=70%\n",
        "- tfidf=70%\n",
        "- word2vec=33%\n",
        "- doc2vec=35%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwtZ0MegFbWg",
        "colab_type": "text"
      },
      "source": [
        "### 4.2.Support Vector Machine (SVM)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "en7vF4vcFbWg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import svm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sr57vBb5FbWi",
        "colab_type": "text"
      },
      "source": [
        "##### SVM using Bag-of-Words Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pABptDhnFbWi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "svc = svm.SVC(kernel='linear', C=1, probability=True).fit(xtrain_bow, ytrain)\n",
        "prediction = svc.predict_proba(xvalid_bow)\n",
        "prediction_class = svc.predict(xvalid_bow)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQ6Hb7mbFbWl",
        "colab_type": "code",
        "colab": {},
        "outputId": "2aade80c-3f70-4580-db3a-80dfb69fabb1"
      },
      "source": [
        "accuracy_score(yvalid, prediction_class)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5471956224350205"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVQB0C2ZFbWn",
        "colab_type": "text"
      },
      "source": [
        "##### SVM using TF-IDF Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlyLu38TFbWn",
        "colab_type": "code",
        "colab": {},
        "outputId": "f464dcf9-ac1c-4cb8-e17e-0f7f45e7ea50"
      },
      "source": [
        "svc = svm.SVC(kernel='linear',C=1, probability=True).fit(xtrain_tfidf, ytrain)\n",
        "prediction = svc.predict_proba(xvalid_tfidf)\n",
        "prediction_class = svc.predict(xvalid_tfidf)\n",
        "accuracy_score(yvalid, prediction_class)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.560875512995896"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PV_qfYBqFbWo",
        "colab_type": "text"
      },
      "source": [
        "##### SVM using word2vec Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jS45LSgBFbWp",
        "colab_type": "code",
        "colab": {},
        "outputId": "ab34060c-32f2-4813-c0ad-e94ab58bf553"
      },
      "source": [
        "svc = svm.SVC(kernel='linear', C=1, probability=True).fit(xtrain_word2vec, ytrain)\n",
        "prediction = svc.predict_proba(xvalid_word2vec)\n",
        "prediction_class = svc.predict(xvalid_word2vec)\n",
        "accuracy_score(yvalid, prediction_class)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.585499316005472"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ieV0FKFFbWr",
        "colab_type": "text"
      },
      "source": [
        "##### SVM using doc2vec Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iU64CelSFbWr",
        "colab_type": "code",
        "colab": {},
        "outputId": "c18edda2-c08f-407d-a61b-a37a2ff2dd23"
      },
      "source": [
        "svc = svm.SVC(kernel='linear', C=1, probability=True).fit(xtrain_doc2vec, ytrain)\n",
        "prediction = svc.predict_proba(xvalid_doc2vec)\n",
        "prediction_class = svc.predict(xvalid_doc2vec)\n",
        "accuracy_score(yvalid, prediction_class)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4117647058823529"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgh6Ar7GFbWt",
        "colab_type": "text"
      },
      "source": [
        "**Summary**\n",
        "\n",
        "\n",
        "- bow = 54%\n",
        "- tfidf= 56%\n",
        "- word2vec= 58% \n",
        "- doc2vec= 41%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8aNqsLFFbWt",
        "colab_type": "text"
      },
      "source": [
        "### 4.3.Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ps80pvJFbWt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWQK6PgIFbWv",
        "colab_type": "text"
      },
      "source": [
        "##### RF with Bag-of-Words Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdTIiauAFbWv",
        "colab_type": "code",
        "colab": {},
        "outputId": "4e1a557e-eda1-4e01-ae3a-9ebea4a656e2"
      },
      "source": [
        "rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_bow, ytrain)\n",
        "prediction = rf.predict_proba(xvalid_bow)\n",
        "prediction_class = rf.predict(xvalid_bow)\n",
        "accuracy_score(yvalid, prediction_class)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5704514363885089"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6eFxNneFbWw",
        "colab_type": "text"
      },
      "source": [
        "##### RF with TF-IDF Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifOtWDDcFbWx",
        "colab_type": "code",
        "colab": {},
        "outputId": "dc253746-78d8-44e0-bee6-17758c10d98e"
      },
      "source": [
        "rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_tfidf, ytrain)\n",
        "prediction = rf.predict_proba(xvalid_tfidf)\n",
        "prediction_class = rf.predict(xvalid_tfidf)\n",
        "accuracy_score(yvalid, prediction_class)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5677154582763337"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkqeNM_FFbWy",
        "colab_type": "text"
      },
      "source": [
        "##### RF with word2vec Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dUPwEfmFbWz",
        "colab_type": "code",
        "colab": {},
        "outputId": "d9c26db0-b2e7-40e4-da40-469661399ecd"
      },
      "source": [
        "rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_word2vec, ytrain)\n",
        "prediction= rf.predict_proba(xvalid_word2vec)\n",
        "prediction_class = rf.predict(xvalid_word2vec)\n",
        "accuracy_score(yvalid, prediction_class)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5745554035567716"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pcw9zWdoFbW2",
        "colab_type": "text"
      },
      "source": [
        "##### RF with doc2vec Feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NCuclqkFbW3",
        "colab_type": "code",
        "colab": {},
        "outputId": "16e04788-dc05-4df2-e1c2-7939c6c6e389"
      },
      "source": [
        "rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_doc2vec, ytrain)\n",
        "prediction= rf.predict_proba(xvalid_doc2vec)\n",
        "prediction_class = rf.predict(xvalid_doc2vec)\n",
        "accuracy_score(yvalid, prediction_class)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4117647058823529"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvk7yT46FbW4",
        "colab_type": "text"
      },
      "source": [
        "**Summary**\n",
        "\n",
        "- bow = 57%\n",
        "- tfidf = 56%\n",
        "- word2vec = 57%\n",
        "- doc2vec = 41%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tKP8YG3FbW4",
        "colab_type": "text"
      },
      "source": [
        "# 4.4.XGBoost\n",
        "Extreme Gradient Boosting (xgboost) is an advanced implementation of gradient boosting algorithm. It has both linear model solver and tree learning algorithms. Its ability to do parallel computation on a single machine makes it extremely fast. It also has additional features for doing cross validation and finding important variables. There are many parameters which need to be controlled to optimize the model.\n",
        "\n",
        "Some key benefits of XGBoost are:\n",
        "\n",
        "Regularization - helps in reducing overfitting\n",
        "Parallel Processing - XGBoost implements parallel processing and is blazingly faster as compared to GBM.\n",
        "Handling Missing Values - It has an in-built routine to handle missing values.\n",
        "Built-in Cross-Validation - allows user to run a cross-validation at each iteration of the boosting process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16EKMob3FbW5",
        "colab_type": "text"
      },
      "source": [
        "**Notice there is no sklearn ready made model therefore; I needed to use XGBoost from its main librrary**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWqxV7jDFbW5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from xgboost import XGBClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YjsO2nTFbW7",
        "colab_type": "text"
      },
      "source": [
        "##### XGBoost using bag of words features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6h2H3WvzFbW7",
        "colab_type": "code",
        "colab": {},
        "outputId": "7830c093-3d5e-4688-9c65-e841e5bf3b56"
      },
      "source": [
        "xgb_model = XGBClassifier(max_depth=6, n_estimators=1000).fit(xtrain_bow, ytrain)\n",
        "prediction = xgb_model.predict_proba(xvalid_bow)\n",
        "prediction_class = xgb_model.predict(xvalid_bow)\n",
        "accuracy_score(yvalid, prediction_class)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5444596443228454"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_ibf9EJFbW9",
        "colab_type": "text"
      },
      "source": [
        "##### XGBoost using tfidf features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpVDlVSEFbW9",
        "colab_type": "code",
        "colab": {},
        "outputId": "789f9bd3-7d8b-4b9e-d7b5-81c79ef382dc"
      },
      "source": [
        "xgb_model = XGBClassifier(max_depth=6, n_estimators=1000).fit(xtrain_tfidf, ytrain)\n",
        "prediction = xgb_model.predict_proba(xvalid_tfidf)\n",
        "prediction_class = xgb_model.predict(xvalid_tfidf)\n",
        "accuracy_score(yvalid, prediction_class)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5554035567715458"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ipUywHkFbXA",
        "colab_type": "text"
      },
      "source": [
        "##### XGBoost using word2vecfeatures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifRSbsHzFbXA",
        "colab_type": "code",
        "colab": {},
        "outputId": "eb3f0e36-9744-4674-da88-63a51a21f4bb"
      },
      "source": [
        "xgb_model = XGBClassifier(max_depth=6, n_estimators=1000).fit(xtrain_word2vec, ytrain)\n",
        "prediction = xgb_model.predict_proba(xvalid_word2vec)\n",
        "prediction_class = xgb_model.predict(xvalid_word2vec)\n",
        "accuracy_score(yvalid, prediction_class)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5991792065663475"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQHzF28HFbXC",
        "colab_type": "text"
      },
      "source": [
        "##### XGBoost using doc2vec features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZucuGahBFbXD",
        "colab_type": "code",
        "colab": {},
        "outputId": "124f9e18-1d95-4f5c-9917-dc6ba9e601a9"
      },
      "source": [
        "xgb_model = XGBClassifier(max_depth=6, n_estimators=1000).fit(xtrain_doc2vec, ytrain)\n",
        "prediction = xgb_model.predict_proba(xvalid_doc2vec)\n",
        "prediction_class = xgb_model.predict(xvalid_doc2vec)\n",
        "accuracy_score(yvalid, prediction_class)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3679890560875513"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICbRc5T6FbXF",
        "colab_type": "text"
      },
      "source": [
        "**Summary**\n",
        "\n",
        "- bow = 54%\n",
        "- tfidf = 55%\n",
        "- word2vec = 58%\n",
        "- doc2vec = 37%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26F5gNOVFbXF",
        "colab_type": "text"
      },
      "source": [
        "### 4.5.MLPClassifier\n",
        "\n",
        "A multilayer perceptron (MLP) is a class of feedforward artificial neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8V451n06FbXF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30cm6SwJFbXH",
        "colab_type": "text"
      },
      "source": [
        "##### MLP using bag of words features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lS4_uEoYFbXI",
        "colab_type": "code",
        "colab": {},
        "outputId": "e815b16c-d25e-4857-fbb7-bf85f02abc47"
      },
      "source": [
        "mlp_model = MLPClassifier(random_state=1, max_iter=300,learning_rate_init=0.001).fit(xtrain_bow, ytrain)\n",
        "prediction = mlp_model.predict_proba(xvalid_bow)\n",
        "prediction_class = mlp_model.predict(xvalid_bow)\n",
        "accuracy_score(yvalid, prediction_class)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.518467852257182"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxnfX4seFbXK",
        "colab_type": "text"
      },
      "source": [
        "##### MLP using tfidf features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySBkf5YeFbXK",
        "colab_type": "code",
        "colab": {},
        "outputId": "6b027b85-3386-45c3-8927-acb9aad02651"
      },
      "source": [
        "mlp_model = MLPClassifier(random_state=1, max_iter=300,learning_rate_init=0.001).fit(xtrain_tfidf, ytrain)\n",
        "prediction = mlp_model.predict_proba(xvalid_tfidf)\n",
        "prediction_class = mlp_model.predict(xvalid_tfidf)\n",
        "accuracy_score(yvalid, prediction_class)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5280437756497948"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "az1Jci6bFbXN",
        "colab_type": "text"
      },
      "source": [
        "##### MLP using word2vecfeatures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnICqUl0FbXN",
        "colab_type": "code",
        "colab": {},
        "outputId": "e0d3e43e-6515-4c23-826d-011eea64c67e"
      },
      "source": [
        "mlp_model = MLPClassifier(random_state=1, max_iter=300,learning_rate_init=0.001).fit(xtrain_word2vec, ytrain)\n",
        "prediction = mlp_model.predict_proba(xvalid_word2vec)\n",
        "prediction_class = mlp_model.predict(xvalid_word2vec)\n",
        "accuracy_score(yvalid, prediction_class)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5595075239398085"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQVsiDu7FbXP",
        "colab_type": "text"
      },
      "source": [
        "##### MLP using doc2vec features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eufGRspKFbXP",
        "colab_type": "code",
        "colab": {},
        "outputId": "4e81ce28-71fb-4c06-a8ef-6bd74cb5fc18"
      },
      "source": [
        "mlp_model = MLPClassifier(random_state=1, max_iter=300,learning_rate_init=0.001).fit(xtrain_doc2vec, ytrain)\n",
        "prediction = mlp_model.predict_proba(xvalid_doc2vec)\n",
        "prediction_class = mlp_model.predict(xvalid_doc2vec)\n",
        "accuracy_score(yvalid, prediction_class)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4117647058823529"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRxshQqqFbXQ",
        "colab_type": "text"
      },
      "source": [
        "**Summary**\n",
        "\n",
        "- bow = 51%\n",
        "- tfidf = 52%\n",
        "- word2vec = 54%\n",
        "- doc2vec = 41%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ve_w4LiQFbXR",
        "colab_type": "text"
      },
      "source": [
        "**XGBoost using word2vec gives us the best results with our given matrics i.e 58%.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_PCOwDkFbXS",
        "colab_type": "text"
      },
      "source": [
        "## Saving model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FD28dlgUFbXS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "xgb_model_best = XGBClassifier(max_depth=6, n_estimators=1000).fit(xtrain_word2vec, ytrain)\n",
        "# save model\n",
        "filename = 'xgb_model.sav'\n",
        "pickle.dump(xgb_model_best, open(filename, 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k52M0FWPFbXU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the model from disk\n",
        "loaded_model = pickle.load(open(filename, 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeSW6WaYFbXW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction = loaded_model.predict_proba(xvalid_word2vec)\n",
        "prediction_class = loaded_model.predict(xvalid_word2vec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3xswhD-FbXY",
        "colab_type": "code",
        "colab": {},
        "outputId": "2b1cb3cf-ed44-4e76-8a6d-2217b88f69db"
      },
      "source": [
        "accuracy_score(yvalid, prediction_class)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5991792065663475"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xA27mwXFFbXZ",
        "colab_type": "code",
        "colab": {},
        "outputId": "75164cac-8550-473f-d6a8-b35581c0edb0"
      },
      "source": [
        "print(classification_report(yvalid, prediction_class))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                      precision    recall  f1-score   support\n",
            "\n",
            "                                   Customer feedback       0.61      0.53      0.57        78\n",
            "                       Data protection (Datenschutz)       1.00      0.50      0.67         4\n",
            "                                   Discovery voucher       0.00      0.00      0.00         4\n",
            "                                           Marketing       0.71      0.52      0.60        23\n",
            "                                    Order management       0.65      0.83      0.73       301\n",
            "                                 Payment (Bezahlung)       0.00      0.00      0.00        12\n",
            "                                   Product (Produkt)       0.71      0.26      0.38        19\n",
            "                                   Production delays       0.00      0.00      0.00         9\n",
            "                    Professional area (Profibereich)       0.60      0.18      0.27        17\n",
            "                                   Reseller workflow       1.00      0.75      0.86         4\n",
            "                                         Rücksendung       1.00      0.14      0.25         7\n",
            "                                       ShareWithSaal       0.66      0.74      0.69        57\n",
            "                                     Shipping issues       0.75      0.19      0.30        32\n",
            "                                Software/Webshop/App       0.49      0.38      0.42        88\n",
            "                                  Special conditions       0.75      0.33      0.46         9\n",
            "   product complaints - colours (Reklamation Farben)       0.67      0.11      0.19        18\n",
            "product complaints - products (Reklamation Produkte)       0.35      0.72      0.47        47\n",
            "product complaints - software (Reklamation Software)       0.00      0.00      0.00         2\n",
            "\n",
            "                                            accuracy                           0.60       731\n",
            "                                           macro avg       0.55      0.34      0.38       731\n",
            "                                        weighted avg       0.60      0.60      0.57       731\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yx8SeJXuFbXb",
        "colab_type": "code",
        "colab": {},
        "outputId": "77449d95-ec18-4f3d-9126-bc2029809e32"
      },
      "source": [
        "print(labels.nunique())\n",
        "print(yvalid.nunique())\n",
        "print(ytrain.nunique())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22\n",
            "18\n",
            "22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qd_qqfkiFbXc",
        "colab_type": "text"
      },
      "source": [
        "Other values are also very consistent.\n",
        "\n",
        "- accuracy = 57.8%\n",
        "- precision = 58%\n",
        "- recall = 58%\n",
        "- f-score = 55%\n",
        "- (test samples=731)\n",
        "- No. of classes in test data = 18\n",
        "- No. of classes in train data = 22\n",
        "- Total Classes = 22"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ioVLZ3HFbXd",
        "colab_type": "text"
      },
      "source": [
        "# END OF NOTEBOOK CODE"
      ]
    }
  ]
}